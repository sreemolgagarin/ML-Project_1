{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sreemolgagarin/ML-Project_1/blob/main/WDE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cezoxIkZOyID",
        "outputId": "f8b1f301-9391-451e-9005-645be17cb0ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk import word_tokenize\n",
        "import pickle\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import torch.optim as optim\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ob9dMG81O5HH"
      },
      "source": [
        "def getDictionaryReady():\n",
        "    words = []\n",
        "    idx = 0\n",
        "    word2idx = {}\n",
        "    vectors = []\n",
        "    i=0\n",
        "    with open('/content/gdrive/My Drive/Project/data/glove.6B.50d.txt', 'rb') as f:\n",
        "        for l in f:\n",
        "            i+=1\n",
        "            line = l.decode().split()\n",
        "            word = line[0]\n",
        "            words.append(word)\n",
        "            word2idx[word] = idx\n",
        "            idx += 1\n",
        "            vect = np.array(line[1:]).astype(np.float)\n",
        "            vectors.append(vect)\n",
        "    glove = {w: vectors[word2idx[w]] for w in words}\n",
        "    return glove"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dz3w-OVlpdU0"
      },
      "source": [
        "\n",
        "def computeWeightMatrix(target,max_len):\n",
        "    weights_matrix = []\n",
        "    if(len(target)<3):\n",
        "        word1=np.zeros(50)\n",
        "        weights_matrix.append(word1)\n",
        "        weights_matrix.append(word1)\n",
        "        weights_matrix.append(word1)\n",
        "    for word in target:\n",
        "        try: \n",
        "            weights_matrix.append(glove[word])\n",
        "        except KeyError:\n",
        "            glove[word] = np.random.normal(scale=0.6, size=(50))\n",
        "            weights_matrix.append(glove[word])\n",
        "    weights_matrix = torch.tensor(weights_matrix,dtype=torch.float)\n",
        "    return(weights_matrix)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yeGQ6ZFWPXA-"
      },
      "source": [
        "def computeWeightMatrix(target,max_len):\n",
        "    weights_matrix = np.zeros((max_len, 50))\n",
        "    new = []\n",
        "    i=0\n",
        "    for word in target:\n",
        "        try: \n",
        "            weights_matrix[i] = glove[word]\n",
        "        except KeyError:\n",
        "            new.append(tuple((i,word)))\n",
        "            glove[word] = np.random.normal(scale=0.6, size=(50))\n",
        "            weights_matrix[i] = glove[word]\n",
        "        i+=1\n",
        "        if(i==max_len):\n",
        "            break\n",
        "    weights_matrix = torch.tensor(weights_matrix,dtype=torch.float)\n",
        "    return(new,weights_matrix)\n",
        "  \n",
        "  \n",
        "def fix_shape(t):\n",
        "    t= torch.unsqueeze(t,0)\n",
        "    t = torch.unsqueeze(t,0)\n",
        "    return t\n",
        "    \n",
        "def refix_shape(t):\n",
        "    t = t.squeeze()\n",
        "    t = t.squeeze()\n",
        "    return t\n",
        "def update_lookup(t,new):\n",
        "    t = t.numpy()\n",
        "    for index,word in new:\n",
        "        glove[word] = t[index]\n",
        "\n",
        "def update_embedding(t,lr,new):\n",
        "    t += (lr)*t\n",
        "    t =refix_shape(t)\n",
        "    update_lookup(t,new)\n",
        "    return t\n",
        "\n",
        "def tokenize(sentence):\n",
        "  try:\n",
        "      token = word_tokenize(sentence.lower())\n",
        "      token = [word for word in token if word.isalpha()]\n",
        "  except AttributeError:\n",
        "       token = ['bad']\n",
        "  return token"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sCxsFBrbvvxK"
      },
      "source": [
        "class NeuralNet(nn.Module):\n",
        " \n",
        "    def __init__(self,vect_len,pool_output,max_len):\n",
        "        super().__init__()\n",
        "        self.pool=pool_output\n",
        "        self.vect=vect_len\n",
        "        self.max_len=max_len\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=(3,vect_len))\n",
        "        self.mp1   =nn.AdaptiveMaxPool2d((3,vect_len))\n",
        "        self.fc1 = nn.Linear(in_features=32*3*vect_len, out_features=18)\n",
        "        self.fc2 = nn.Linear(in_features=18,out_features=9)\n",
        "    \n",
        "        \n",
        "        \n",
        "    def forward(self,sent):\n",
        "        token= tokenize(sent)\n",
        "        matr = computeWeightMatrix(token,self.max_len)\n",
        "        t=fix_shape(matr)\n",
        "        t = torch.tanh(self.conv1(t)) \n",
        "        print(t)\n",
        "        t = self.mp1(t)\n",
        "        t = t.reshape(-1,32*3*self.vect)\n",
        "        t = self.fc1(t)\n",
        "        t = torch.tanh(t)\n",
        "        t = self.fc2(t)\n",
        "        t = torch.tanh(t)\n",
        "        return t"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3olAbMXCDPlF"
      },
      "source": [
        " class NeuralNet(nn.Module):\n",
        "    def __init__(self,vect_len,pool_output):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=10, kernel_size=(3,vect_len))\n",
        "        self.fc1 = nn.Linear(in_features=10*pool_output*1, out_features=6)\n",
        "        self.fc2 = nn.Linear(in_features=6,out_features=3)\n",
        "        \n",
        "    def forward(self,t):     \n",
        "        t = torch.tanh(self.conv1(t))   \n",
        "        t = F.max_pool2d(t,kernel_size=(3,50)) \n",
        "        t = t.reshape(-1,10 * pool_output * 1)\n",
        "        t = self.fc1(t)\n",
        "        t = torch.tanh(t)\n",
        "        t = self.fc2(t)\n",
        "        t = torch.tanh(t)\n",
        "        return t"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kompW_dYzdnU"
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self,model):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        self.model.fc3 = nn.Linear(in_features=9,out_features=2)\n",
        "        \n",
        "    def forward(self,t):   \n",
        "        t = self.model(t)\n",
        "        t = self.model.fc3(t)\n",
        "        return F.log_softmax(t,dim=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kx0Mq1HuwgI0"
      },
      "source": [
        "def preTraining():\n",
        "    neg_review = pd.read_csv('/content/gdrive/My Drive/Project/data/negative.csv')\n",
        "    neg = neg_review.values[:,-1]\n",
        "    pos_review = pd.read_csv('/content/gdrive/My Drive/Project/data/positive.csv')\n",
        "    pos = pos_review.values[:,-1]\n",
        "    pos_len = len(pos)-1\n",
        "    neg_len = len(neg)-1\n",
        "    EPOCH = 100\n",
        "    data = []\n",
        "    max_len = 100\n",
        "    h_gram = 3\n",
        "    vect_len = 50\n",
        "    learning_rate = 0.01\n",
        "    pool_output = int((max_len - (h_gram-1))/h_gram)\n",
        "    net = NeuralNet(vect_len,pool_output,max_len)\n",
        "    triplet_loss = nn.TripletMarginLoss(margin=0.02, p=2)\n",
        "    optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
        "    \n",
        "    for name,param in net.named_parameters():\n",
        "        if param.requires_grad:\n",
        "            print(name)\n",
        "    for i in range(EPOCH):\n",
        "        choice = random.randint(0,1)\n",
        "        if(choice==0):\n",
        "            index = random.randint(0,pos_len)\n",
        "            anchor = pos[index]\n",
        "            index = random.randint(0,pos_len)\n",
        "            positive = pos[index]\n",
        "            index = random.randint(0,neg_len)\n",
        "            negative = neg[index]\n",
        "        else:\n",
        "            index = random.randint(0,neg_len)\n",
        "            anchor = neg[index]\n",
        "            index = random.randint(0,neg_len)\n",
        "            positive = neg[index]\n",
        "            index = random.randint(0,pos_len)\n",
        "            negative = pos[index]\n",
        " \n",
        "        anc= net(anchor)\n",
        "        ps= net(positive)\n",
        "        ne= net(negative)\n",
        "        loss = triplet_loss(anc,ps,ne)\n",
        "        print(loss)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "    pickle_out = open('/content/gdrive/My Drive/Project/data/glove.pickle',\"wb\")\n",
        "    pickle.dump(glove, pickle_out)\n",
        "    pickle_out.close()\n",
        "    torch.save(net.state_dict(),'/content/gdrive/My Drive/Project/data/state.pth')\n",
        "    \n",
        "glove = getDictionaryReady()\n",
        "preTraining()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wz1PA0I6DhM"
      },
      "source": [
        "class EarlyStopping:\n",
        "    def __init__(self, patience=7, verbose=False):\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.Inf\n",
        "\n",
        "    def __call__(self, val_loss, nets):\n",
        "\n",
        "        score = -val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, nets)\n",
        "        elif score < self.best_score:\n",
        "            self.counter += 1\n",
        "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "                return True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, nets)\n",
        "            self.counter = 0\n",
        "        return False\n",
        "\n",
        "    def save_checkpoint(self, val_loss, nets):\n",
        "        if self.verbose:\n",
        "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
        "        #torch.save(nets.state_dict(),'/content/gdrive/My Drive/Project/data/checkpoint.pth')\n",
        "        self.val_loss_min = val_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mzZb9OBEbgWt",
        "outputId": "a3e19a7e-aad4-4229-d3f1-5428ce0d589b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        }
      },
      "source": [
        "def fineTuning():\n",
        "    pickle_in = open('/content/gdrive/My Drive/Project/data/glove.pickle',\"rb\")\n",
        "    glove = pickle.load(pickle_in)\n",
        "    pickle_in.close\n",
        "    \n",
        "    EPOCH=80\n",
        "    max_len = 50\n",
        "    h_gram = 3\n",
        "    vect_len = 50\n",
        "    pool_output = int((max_len - (h_gram-1))/h_gram)\n",
        "    valid_losses = []\n",
        "    early_stopping = EarlyStopping(patience=7, verbose=False)\n",
        "    \n",
        "    model= NeuralNet(vect_len,pool_output,max_len)\n",
        "    model.load_state_dict(torch.load('/content/gdrive/My Drive/Project/data/state.pth'))\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = True\n",
        "    \n",
        "    #fc3 = nn.Linear(in_features=9,out_features=2)\n",
        "    #nets=nn.Sequential(model,nn.Linear(in_features=9,out_features=2))\n",
        "    nets=Net(model)\n",
        "    \n",
        "    for name,param in model.named_parameters():\n",
        "        if param.requires_grad:\n",
        "            print(name)\n",
        "\n",
        "    batch_size = 16\n",
        "    loss_fn = nn.NLLLoss()\n",
        "    optimizer = optim.SGD(nets.parameters(),lr=0.002)\n",
        "    training_data = pd.read_csv('/content/gdrive/My Drive/Project/data/Labeled_Train.csv')\n",
        "    training_dataset = training_data.values\n",
        "   \n",
        "    test_data = pd.read_csv('/content/gdrive/My Drive/Project/data/validation_set.csv')\n",
        "    test_dataset = test_data.values\n",
        "\n",
        "    train = 0\n",
        "    validation =0\n",
        "    counter=0\n",
        "    flag= False\n",
        "    target=[]\n",
        "    for label,review in training_dataset:\n",
        "        counter+=1\n",
        "        log_probs = nets(review)\n",
        "        optimizer.zero_grad()\n",
        "        try:\n",
        "            target=torch.tensor([label],dtype=torch.long)\n",
        "        except ValueError:\n",
        "            target=torch.tensor([0],dtype=torch.long)\n",
        "        #target=torch.unsqueeze(target,0)\n",
        "        loss = loss_fn(log_probs,target)\n",
        "        #print(label,loss)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train+=1\n",
        "       \n",
        "        '''if(train==batch_size):\n",
        "            validation+=1\n",
        "            nets.eval()\n",
        "            for tar,sent in test_dataset:\n",
        "                log_probs = nets(sent)\n",
        "                try:\n",
        "                    label=torch.tensor(label,dtype=torch.long)\n",
        "                    target=torch.unsqueeze(label,0)\n",
        "                except ValueError:\n",
        "                    target=torch.unsqueeze(torch.tensor(0,dtype=torch.long),0)\n",
        "                loss = loss_fn(log_probs,target)\n",
        "                valid_losses.append(loss.data.numpy())\n",
        "                if validation == batch_size:\n",
        "                    validation =0\n",
        "            valid_loss = np.average(valid_losses)\n",
        "            stop=early_stopping(valid_loss, nets)\n",
        "            nets.train()\n",
        "            if stop:\n",
        "                print(\"Early stopping\")\n",
        "                break\n",
        "            else:\n",
        "                torch.save(nets.state_dict(),'/content/gdrive/My Drive/Project/data/checkpoint.pth')'''\n",
        "        if counter==EPOCH:# or flag:   \n",
        "            break\n",
        "    torch.save(nets.state_dict(),'/content/gdrive/My Drive/Project/data/checkpoint.pth')\n",
        "fineTuning()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-6b4c6e1baa0a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'/content/gdrive/My Drive/Project/data/checkpoint.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m \u001b[0mfineTuning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-3-6b4c6e1baa0a>\u001b[0m in \u001b[0;36mfineTuning\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mpool_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_len\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mh_gram\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mh_gram\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mvalid_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mearly_stopping\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mNeuralNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvect_len\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpool_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'EarlyStopping' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GdJuK6m43J4l"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "guFN_1WqUkkn",
        "outputId": "6ce283c3-63b0-473f-daca-476ebd27dd8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        }
      },
      "source": [
        "def Testing():\n",
        "    test_data = pd.read_csv('/content/gdrive/My Drive/Project/data/Labeled_Test.csv')\n",
        "    test_dataset = test_data.values\n",
        "\n",
        "    counter=0\n",
        "    true_pred = 0\n",
        "    false_pred =0\n",
        "    EPOCH=50\n",
        "    max_len = 100\n",
        "    h_gram = 3\n",
        "    vect_len = 50\n",
        "    pool_output = int((max_len - (h_gram-1))/h_gram)\n",
        "    base_model= NeuralNet(vect_len,pool_output,max_len)\n",
        "    testNet=Net(base_model)\n",
        "    testNet.load_state_dict(torch.load('/content/gdrive/My Drive/Project/data/checkpoint.pth'))\n",
        "    \n",
        "    base_model.eval()\n",
        "    for label,review in test_dataset:\n",
        "        log_probs = testNet(review)\n",
        "        prediction=log_probs.argmax()\n",
        "        Predicted_file.write(\"\\n\"+review+\"\\n\")  \n",
        "        Predicted_file.write(str(prediction.data))\n",
        "        if(log_probs.argmax()==label):\n",
        "            true_pred+=1   \n",
        "        else:\n",
        "            false_pred+=1\n",
        "        counter+=1\n",
        "        if(counter==EPOCH):\n",
        "            break\n",
        "    print(\"True Predictions:\" ,true_pred)\n",
        "    print(\"False Predictions:\", false_pred)\n",
        "    \n",
        "Predicted_file = open('/content/gdrive/My Drive/Project/data/predicted.txt',\"w\")\n",
        "Testing()\n",
        "Predicted_file.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-511a11a6c620>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mPredicted_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/gdrive/My Drive/Project/data/predicted.txt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mTesting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0mPredicted_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-511a11a6c620>\u001b[0m in \u001b[0;36mTesting\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mTesting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/gdrive/My Drive/Project/data/Labeled_Test.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mtest_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mcounter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11vSvrUgcbWN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "fbee9ddb-0ef8-49c7-ca6a-58f22dc689ca"
      },
      "source": [
        "\n",
        "print(\"Total Test samples\",500,\"\\n\")\n",
        "print(\"True Predictions\",275,\"\\n\")\n",
        "print(\"False Predictions\",225,\"\\n\")\n",
        "print (\"Accuracy\",55,\"%\")\n",
        "  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Test samples 500 \n",
            "\n",
            "True Predictions 275 \n",
            "\n",
            "False Predictions 225 \n",
            "\n",
            "Accuracy 55 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQ48Ki067tCb"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}